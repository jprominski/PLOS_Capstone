<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">28820891</article-id><article-id pub-id-type="pmc">5562312</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0182227</article-id><article-id pub-id-type="publisher-id">PONE-D-17-12825</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied Mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Simulation and Modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and Technology</subject><subj-group><subject>Signal Processing</subject><subj-group><subject>Quantization (Signal Processing)</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural Networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and Information Sciences</subject><subj-group><subject>Computer Vision</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Neurology</subject><subj-group><subject>Cerebrovascular Diseases</subject><subj-group><subject>Stroke</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and Health Sciences</subject><subj-group><subject>Vascular Medicine</subject><subj-group><subject>Stroke</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and Analysis Methods</subject><subj-group><subject>Imaging Techniques</subject><subj-group><subject>Computer Imaging</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive Science</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social Sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive Psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and Life Sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and Memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Scene text detection via extremal region based double threshold convolutional network classification</article-title><alt-title alt-title-type="running-head">Scene text detection via MSER based double threshold CNN classification</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhu</surname><given-names>Wei</given-names></name><role content-type="http://credit.casrai.org/">Conceptualization</role><role content-type="http://credit.casrai.org/">Methodology</role><role content-type="http://credit.casrai.org/">Writing &#x02013; original draft</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff001"/></contrib><contrib contrib-type="author"><name><surname>Lou</surname><given-names>Jing</given-names></name><role content-type="http://credit.casrai.org/">Supervision</role><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff001"/></contrib><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Longtao</given-names></name><role content-type="http://credit.casrai.org/">Writing &#x02013; review &#x00026; editing</role><xref ref-type="aff" rid="aff001"/></contrib><contrib contrib-type="author"><name><surname>Xia</surname><given-names>Qingyuan</given-names></name><role content-type="http://credit.casrai.org/">Funding acquisition</role><xref ref-type="aff" rid="aff001"/></contrib><contrib contrib-type="author"><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-5576-3281</contrib-id><name><surname>Ren</surname><given-names>Mingwu</given-names></name><role content-type="http://credit.casrai.org/">Supervision</role><xref ref-type="corresp" rid="cor001">*</xref><xref ref-type="aff" rid="aff001"/></contrib></contrib-group><aff id="aff001"><addr-line>School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu, China</addr-line></aff><contrib-group><contrib contrib-type="editor"><name><surname>Wang</surname><given-names>Yuanquan</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1"><addr-line>Beijing University of Technology, CHINA</addr-line></aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p></fn><corresp id="cor001">* E-mail: <email>mingwuren@163.com</email></corresp></author-notes><pub-date pub-type="epub"><day>18</day><month>8</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>12</volume><issue>8</issue><elocation-id>e0182227</elocation-id><history><date date-type="received"><day>2</day><month>4</month><year>2017</year></date><date date-type="accepted"><day>16</day><month>7</month><year>2017</year></date></history><permissions><copyright-statement>&#x000a9; 2017 Zhu et al</copyright-statement><copyright-year>2017</copyright-year><copyright-holder>Zhu et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="pone.0182227.pdf"/><abstract><p>In this paper, we present a robust text detection approach in natural images which is based on region proposal mechanism. A powerful low-level detector named saliency enhanced-MSER extended from the widely-used MSER is proposed by incorporating saliency detection methods, which ensures a high recall rate. Given a natural image, character candidates are extracted from three channels in a perception-based illumination invariant color space by saliency-enhanced MSER algorithm. A discriminative convolutional neural network (CNN) is jointly trained with multi-level information including pixel-level and character-level information as character candidate classifier. Each image patch is classified as strong text, weak text and non-text by double threshold filtering instead of conventional one-step classification, leveraging confident scores obtained via CNN. To further prune non-text regions, we develop a recursive neighborhood search algorithm to track credible texts from weak text set. Finally, characters are grouped into text lines using heuristic features such as spatial location, size, color, and stroke width. We compare our approach with several state-of-the-art methods, and experiments show that our method achieves competitive performance on public datasets ICDAR 2011 and ICDAR 2013.</p></abstract><funding-group><award-group id="award001"><funding-source><institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>61231014</award-id><principal-award-recipient><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-5576-3281</contrib-id><name><surname>Ren</surname><given-names>Mingwu</given-names></name></principal-award-recipient></award-group><award-group id="award002"><funding-source><institution-wrap><institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>6140320</award-id><principal-award-recipient><name><surname>Xia</surname><given-names>Qingyuan</given-names></name></principal-award-recipient></award-group><award-group id="award003"><funding-source><institution>China Postdoctoral Science Foundation (CN)</institution></funding-source><award-id>2014M561654</award-id><principal-award-recipient><name><surname>Xia</surname><given-names>Qingyuan</given-names></name></principal-award-recipient></award-group><funding-statement>MR was supported by the National Natural Science Foundation of China under Grant 61231014 (<ext-link ext-link-type="uri" xlink:href="http://www.nsfc.gov.cn/">http://www.nsfc.gov.cn/</ext-link>). QX was supported by the National Natural Science Foundation of China under Grant 6140320 (<ext-link ext-link-type="uri" xlink:href="http://www.nsfc.gov.cn/">http://www.nsfc.gov.cn/</ext-link>), and the China Postdoctoral Science Foundation under Grant No. 2014M561654 (<ext-link ext-link-type="uri" xlink:href="http://www.chinapostdoctor.org.cn">http://www.chinapostdoctor.org.cn</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><fig-count count="10"/><table-count count="3"/><page-count count="17"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All relevant data are available from <ext-link ext-link-type="uri" xlink:href="http://github.com/zw88/SceneTextDetection">http://github.com/zw88/SceneTextDetection</ext-link>.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All relevant data are available from <ext-link ext-link-type="uri" xlink:href="http://github.com/zw88/SceneTextDetection">http://github.com/zw88/SceneTextDetection</ext-link>.</p></notes></front><body><sec sec-type="intro" id="sec001"><title>Introduction</title><p>Reading text in the wild is significant in a variety of advanced computer vision applications, such as image and video retrieval, scene understanding and visual assistance, since text in images usually conveys valuable information. Hence, detection and recognizing text in scene images has received increasing attention in this community. Though extensively studied in recent years, text detection in unconstrained environments is still quite challenging due to a number of factors, such as high variation in character font, size, color, orientation as well as complicated background and non-uniform illumination.</p><p>Previous works for scene text detection based on sliding windows [<xref rid="pone.0182227.ref001" ref-type="bibr">1</xref>&#x02013;<xref rid="pone.0182227.ref005" ref-type="bibr">5</xref>] and connected component analysis [<xref rid="pone.0182227.ref006" ref-type="bibr">6</xref>&#x02013;<xref rid="pone.0182227.ref014" ref-type="bibr">14</xref>] have become mainstream in this domain. Sliding windows based methods localize text regions by shifting a multi-scaled classification window. This exhaustive search is computationally inefficient though it achieves high recall rates. Methods based on connected components extract individual characters through connected component analysis followed by grouping and refinement strategy. Additionally, false alarm removing may be performed to remove non-text components. Stroke Width Transform (SWT) [<xref rid="pone.0182227.ref006" ref-type="bibr">6</xref>] and Maximally Stable Extremal Region (MSER) [<xref rid="pone.0182227.ref015" ref-type="bibr">15</xref>] are two representative techniques, particularly methods based on MSER achieved the state-of-the-art performance on ICDAR2013 and ICDAR2015 competitions [<xref rid="pone.0182227.ref016" ref-type="bibr">16</xref>, <xref rid="pone.0182227.ref017" ref-type="bibr">17</xref>]. However, the MSER algorithms extract massive repeating non-text components which will be constrained by false-removing and refinement rules. These methods are also incapable of detecting characters distorted by noise or background.</p><p>More recently, several deep learning based approaches [<xref rid="pone.0182227.ref005" ref-type="bibr">5</xref>, <xref rid="pone.0182227.ref018" ref-type="bibr">18</xref>&#x02013;<xref rid="pone.0182227.ref024" ref-type="bibr">24</xref>] have been developed for scene text detection owing to deep model feature representations. These models building on convolutional neural networks (CNN) compute high-level deep features from image patches or proposals for text/non-text classification. These methods are also restricted by region proposal methods and the discriminative power of CNN classifiers.</p><p>In this paper, we propose a robust approach which combines the advantages of both MSER and CNN feature representations. Our contributions can be summarized into three points. First, a saliency enhanced-MSER, which is an extension of the well-known MSER algorithm by incorporating saliency detection methods, is proposed as character candidate extractor on three channels of the image to ensure a high recall rate. The second contribution is a novel text filtering pipeline with a deep CNN. In the classification stage, we train a powerful convolutional neural network which incorporates pixel-level and character-level information. The CNN is jointly learned with one main task (i.e., text/non-text classification) and two auxiliary tasks (i.e., text region segmentation and character recognition). With the powerful CNN, we classify the candidates into strong/weak texts and non-texts by applying double threshold filtering. Third, we propose a recursive neighborhood search algorithm to further track texts from strong texts. Finally, we use heuristic rules to construct text lines.</p><p>The rest of the paper is organized as follows. In Section <italic>Previous Work</italic>, a brief overview of related studies is given. Section <italic>Methodology</italic> presents the details of the proposed method. Experimental verifications are presented in Section <italic>Experiments and Results</italic>, and finally the paper is concluded in Section <italic>Conclusions</italic>. The pipeline is shown in <xref ref-type="fig" rid="pone.0182227.g001">Fig 1</xref>.</p><fig id="pone.0182227.g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0182227.g001</object-id><label>Fig 1</label><caption><title>Framework of our proposed algorithm.</title><p>(<bold>A</bold>) Input image [<xref rid="pone.0182227.ref053" ref-type="bibr">53</xref>]. (<bold>B</bold>) I-channel image in the PII color space. (<bold>C</bold>) H-channel image in the PII color space. (<bold>D</bold>) S-channel image in the PII color space. (<bold>E</bold>) Result of SE-MSER algorithm. (<bold>F</bold>) Strong text after CNN classification. (<bold>G</bold>) Weak text after CNN classification. (<bold>H</bold>) Recursive local search and duplicate removal. (<bold>I</bold>) Final result.</p></caption><graphic xlink:href="pone.0182227.g001"/></fig></sec><sec id="sec002"><title>Previous work</title><p>Numerous methods have been developed for text localization in real world images in recent years, which can be roughly categorized into two groups: sliding window based methods and connected component based methods. Sliding window based techniques [<xref rid="pone.0182227.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0182227.ref003" ref-type="bibr">3</xref>, <xref rid="pone.0182227.ref005" ref-type="bibr">5</xref>, <xref rid="pone.0182227.ref025" ref-type="bibr">25</xref>] apply multi-scale windows across the image and a character or a word is checked by a classifier. The main limitation is the heavy computational cost resulted by discriminating a huge number of windows.</p><p>The connected component based (CC-based) methods [<xref rid="pone.0182227.ref004" ref-type="bibr">4</xref>, <xref rid="pone.0182227.ref006" ref-type="bibr">6</xref>&#x02013;<xref rid="pone.0182227.ref015" ref-type="bibr">15</xref>, <xref rid="pone.0182227.ref019" ref-type="bibr">19</xref>, <xref rid="pone.0182227.ref026" ref-type="bibr">26</xref>&#x02013;<xref rid="pone.0182227.ref031" ref-type="bibr">31</xref>] have become increasingly explored in text detection tasks. Stroke Width Transform (SWT) [<xref rid="pone.0182227.ref006" ref-type="bibr">6</xref>] and its variants [<xref rid="pone.0182227.ref004" ref-type="bibr">4</xref>, <xref rid="pone.0182227.ref010" ref-type="bibr">10</xref>, <xref rid="pone.0182227.ref011" ref-type="bibr">11</xref>, <xref rid="pone.0182227.ref013" ref-type="bibr">13</xref>] make use of the property that characters have nearly constant stroke width. These methods are sensitive to noise and blur as they rely on accurate edge detections. The recently most successful methods based on MSER [<xref rid="pone.0182227.ref032" ref-type="bibr">32</xref>] have demonstrated promising performance in the literature. The method presented in [<xref rid="pone.0182227.ref015" ref-type="bibr">15</xref>] detects characters as MSERs followed by a classification process. Neumann and Matas presented a method that considered all extremal regions as character proposals followed by pruning with the exhaustive search strategy. The winning method of ICDAR 2013 [<xref rid="pone.0182227.ref017" ref-type="bibr">17</xref>] in text localization proposed by Yin et al. [<xref rid="pone.0182227.ref029" ref-type="bibr">29</xref>] refines MSER with several pruning techniques and then uses a single-link clustering algorithm to group the characters. The methods in [<xref rid="pone.0182227.ref008" ref-type="bibr">8</xref>, <xref rid="pone.0182227.ref033" ref-type="bibr">33</xref>, <xref rid="pone.0182227.ref034" ref-type="bibr">34</xref>] leverage an inclusion relation amongst ERs called ER tree to extract character candidates. For modeling image patches, Baochang Zhang et al. [<xref rid="pone.0182227.ref035" ref-type="bibr">35</xref>] developed multiple Gaussian uncertainty theory and exploited the application in computer vision tasks.</p><p>Applying proper features to text plays an important role in the following classification step. In early works, methods in [<xref rid="pone.0182227.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0182227.ref002" ref-type="bibr">2</xref>] treat text as a special type of texture and make use of its textural properties, such as local intensities, spatial variance, filter responses and wavelet coefficients. Compared with faces and pedestrians, text-lines in natural images have more variations which cannot be well captured by conventional descriptors. Approaches of [<xref rid="pone.0182227.ref007" ref-type="bibr">7</xref>, <xref rid="pone.0182227.ref010" ref-type="bibr">10</xref>, <xref rid="pone.0182227.ref013" ref-type="bibr">13</xref>, <xref rid="pone.0182227.ref027" ref-type="bibr">27</xref>, <xref rid="pone.0182227.ref030" ref-type="bibr">30</xref>, <xref rid="pone.0182227.ref036" ref-type="bibr">36</xref>] eliminate non-text components using features based on geometric and appearance properties. Neumann and Matas [<xref rid="pone.0182227.ref007" ref-type="bibr">7</xref>] filtered non-text ERs by cascade filtering using geometric features (e.g. bounding box, perimeter, Euler numbers, horizontal crossings, aspect ratio, compactness, etc.). Yao et al. [<xref rid="pone.0182227.ref010" ref-type="bibr">10</xref>] proposed component level features (e.g., contour shape, edge shape, width variation, density, etc.) to further reject false detections. Huang et al. [<xref rid="pone.0182227.ref013" ref-type="bibr">13</xref>] proposed two novel Text Covariance Descriptors (TCDs) that encode both heuristic properties and statistical characteristics of text strokes. More conventional features and their variants such as LBP, DCT and HOG [<xref rid="pone.0182227.ref028" ref-type="bibr">28</xref>, <xref rid="pone.0182227.ref033" ref-type="bibr">33</xref>, <xref rid="pone.0182227.ref034" ref-type="bibr">34</xref>, <xref rid="pone.0182227.ref037" ref-type="bibr">37</xref>, <xref rid="pone.0182227.ref038" ref-type="bibr">38</xref>] have been adopted to train classifiers due to their effectiveness. These features are used to train various classifiers such as SVM, random forest and decision trees [<xref rid="pone.0182227.ref007" ref-type="bibr">7</xref>, <xref rid="pone.0182227.ref028" ref-type="bibr">28</xref>, <xref rid="pone.0182227.ref039" ref-type="bibr">39</xref>, <xref rid="pone.0182227.ref040" ref-type="bibr">40</xref>] or construct dictionaries [<xref rid="pone.0182227.ref041" ref-type="bibr">41</xref>&#x02013;<xref rid="pone.0182227.ref043" ref-type="bibr">43</xref>] for further processing.</p><p>Due to the powerful discrimination ability of deep CNN features, various methods based on CNN have been successfully applied to scene text detection recently [<xref rid="pone.0182227.ref005" ref-type="bibr">5</xref>, <xref rid="pone.0182227.ref018" ref-type="bibr">18</xref>, <xref rid="pone.0182227.ref020" ref-type="bibr">20</xref>, <xref rid="pone.0182227.ref021" ref-type="bibr">21</xref>, <xref rid="pone.0182227.ref023" ref-type="bibr">23</xref>]. Wang et al. [<xref rid="pone.0182227.ref018" ref-type="bibr">18</xref>] employed a traditional CNN model in the sliding window fashion for text detection. In [<xref rid="pone.0182227.ref019" ref-type="bibr">19</xref>, <xref rid="pone.0182227.ref021" ref-type="bibr">21</xref>], Huang et al. proposed a novel framework which integrated MSER and CNN. The MSER works in the front-end to extract text candidates, while a CNN model is employed to filter out non-text components. This algorithm shows great advantage on performance over conventional methods. Method presented in [<xref rid="pone.0182227.ref005" ref-type="bibr">5</xref>] computes a text saliency map by evaluating the character/background CNN classifier in a sliding window fashion across the image. Gupta et al. [<xref rid="pone.0182227.ref023" ref-type="bibr">23</xref>] developed a Fully-Convolutional Regression Network (FCRN) trained with synthetic images which performs both text detection and bounding box regression. A robust object representation which is a fusion of handcraft features and deep learned features is proposed in [<xref rid="pone.0182227.ref044" ref-type="bibr">44</xref>].</p><p>The proposed approach combines the advantages of both text proposal methods and deep CNN models. Despite the success of CC-based methods, we observe that constraints commonly exist in two aspects. First, region proposal techniques are not enough to preserve various true characters, leading to a low recall in practice. Second, text/non-text classifiers are not discriminative enough to reduce the noises in character candidates. Moreover, simply relying on one-step filtering is not robust to detect true texts precisely. Thus, this paper aims to address such limitations.</p></sec><sec sec-type="materials|methods" id="sec003"><title>Methodology</title><p>In this section, we present the details of the proposed algorithm. The full process is separated into three parts: character proposal, text/non-text filtering and text line construction, each of which will be described in details in the next several sections.</p><sec id="sec004"><title>Character candidate extraction</title><sec id="sec005"><title>Color space conversion</title><p>Text is usually perceptually distinct in color from its background, so a color space named perception-based illumination invariant color space which is robust to spectral changes in illumination is used [<xref rid="pone.0182227.ref045" ref-type="bibr">45</xref>]. Let&#x02019;s assume that <inline-formula id="pone.0182227.e001"><alternatives><graphic xlink:href="pone.0182227.e001.jpg" id="pone.0182227.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> is the tristimulus value of sensor represented in <italic>XYZ</italic> coordinates and <inline-formula id="pone.0182227.e002"><alternatives><graphic xlink:href="pone.0182227.e002.jpg" id="pone.0182227.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the 3D color space parameterization. Following [<xref rid="pone.0182227.ref045" ref-type="bibr">45</xref>], the relationship between <inline-formula id="pone.0182227.e003"><alternatives><graphic xlink:href="pone.0182227.e003.jpg" id="pone.0182227.e003g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M3"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0182227.e004"><alternatives><graphic xlink:href="pone.0182227.e004.jpg" id="pone.0182227.e004g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M4"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> can be represented by Eq (<xref ref-type="disp-formula" rid="pone.0182227.e005">1</xref>) as follows:
<disp-formula id="pone.0182227.e005"><alternatives><graphic xlink:href="pone.0182227.e005.jpg" id="pone.0182227.e005g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M5"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover><mml:mrow><mml:mi mathvariant="normal">ln</mml:mi></mml:mrow><mml:mo>&#x02227;</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>B</mml:mi><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="true">&#x02192;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></alternatives><label>(1)</label></disp-formula>
where <italic>A</italic> and <italic>B</italic> are invertible 3&#x000d7;3 matrices and <inline-formula id="pone.0182227.e006"><alternatives><graphic xlink:href="pone.0182227.e006.jpg" id="pone.0182227.e006g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M6"><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="normal">ln</mml:mi></mml:mrow><mml:mo>&#x02227;</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> denotes component-wise natural logarithm. In [<xref rid="pone.0182227.ref045" ref-type="bibr">45</xref>], the matrices <italic>A</italic> and <italic>B</italic> have been experimentally estimated using databases of similar colors and their values are as follows:
<disp-formula id="pone.0182227.e007"><alternatives><graphic xlink:href="pone.0182227.e007.jpg" id="pone.0182227.e007g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M7"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>27.07439</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>22.80783</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1.806681</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>5.646736</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>7.722125</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>12.86503</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>4.163133</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>4.579428</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>4.576049</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives><label>(2)</label></disp-formula>
<disp-formula id="pone.0182227.e008"><alternatives><graphic xlink:href="pone.0182227.e008.jpg" id="pone.0182227.e008g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M8"><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>0.9465229</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>0.2946927</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>0.1313419</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>0.117917</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>0.9929960</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>0.007371554</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0.0923046</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>0.04645794</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>0.9946464</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives><label>(3)</label></disp-formula></p><p>By transforming the tristimulus values of an image according to Eq (<xref ref-type="disp-formula" rid="pone.0182227.e005">1</xref>), one can obtain color descriptors that are approximately invariant to illumination. Therefore, it is intuitive to take advantage of these illumination-invariant color descriptors to extract characters instead of working directly on RGB values. It has been shown in [<xref rid="pone.0182227.ref027" ref-type="bibr">27</xref>] that the PII color space can enhance the robustness of MSER/ER based algorithms.</p></sec><sec id="sec006"><title>MSER component extraction</title><p>Maximally stable extremal region (MSER) [<xref rid="pone.0182227.ref032" ref-type="bibr">32</xref>, <xref rid="pone.0182227.ref046" ref-type="bibr">46</xref>] and its variants have been identified as one of the best character region detectors in recent years and demonstrate remarkable performance [<xref rid="pone.0182227.ref019" ref-type="bibr">19</xref>, <xref rid="pone.0182227.ref029" ref-type="bibr">29</xref>, <xref rid="pone.0182227.ref031" ref-type="bibr">31</xref>]. However, the assumption that texts usually have distinct contrast to its background and uniform intensity or color may not always hold. MSERs detected as text regions are easily distorted by various factors (e.g., low contrast, low resolution, blurring, etc.), which will lead to numerous false detections. In this step, we focus on retrieving text components as many as possible, so a high MSER margin is used and most ERs are employed. Many recent works [<xref rid="pone.0182227.ref007" ref-type="bibr">7</xref>, <xref rid="pone.0182227.ref028" ref-type="bibr">28</xref>, <xref rid="pone.0182227.ref033" ref-type="bibr">33</xref>, <xref rid="pone.0182227.ref047" ref-type="bibr">47</xref>] have exploited multi-channel techniques to enhance the performance of MSER. An experimental validation in [<xref rid="pone.0182227.ref007" ref-type="bibr">7</xref>] shows that the combination of intensity, hue and saturation channels is found as the best trade-off between short run time and localization performance. In this paper, we extract regions on the grayscale, hue and saturation channel images in the PII color space to ensure the recall rate. Multi-channel MSER detection results can be seen in <xref ref-type="fig" rid="pone.0182227.g002">Fig 2</xref>, here we set the MSER threshold to 4 for better display.</p><fig id="pone.0182227.g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0182227.g002</object-id><label>Fig 2</label><caption><title>Multi-channel MSER components extraction.</title><p>(<bold>A</bold>) Original image [<xref rid="pone.0182227.ref053" ref-type="bibr">53</xref>]. (<bold>B</bold>) MSER on gray channel. (<bold>C</bold>) MSER on PII-hue channel. (<bold>D</bold>) MSER on PII-sat channel.</p></caption><graphic xlink:href="pone.0182227.g002"/></fig></sec><sec id="sec007"><title>Saliency-enhanced MSER</title><p>Although MSER operator dramatically reduces the number of windows compared with sliding-window methods, some text regions may be missed or distorted resulting in low recall rate in practice. Employing all ERs gets higher recall, the reason we do not apply this is that it suffers from a much larger number of false detections. It is difficult to recover the missed texts in the subsequent progress, thus we need to further improve the recall of the aforementioned MSER method to find a better trade-off between computational cost and detection performance. Towards this, we propose an efficient approach which incorporates cluster-based and histogram-based saliency detection method to enhance region contrast of natural images.</p><p>Motivated by cluster-based saliency detection method in [<xref rid="pone.0182227.ref048" ref-type="bibr">48</xref>], we first compute contrast cues from the image. Given an image <italic>I</italic>, we obtain <italic>K</italic> clusters <inline-formula id="pone.0182227.e009"><alternatives><graphic xlink:href="pone.0182227.e009.jpg" id="pone.0182227.e009g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M9"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> using K-means algorithm. The contrast cue <italic>w</italic>(<italic>k</italic>) of cluster <italic>C</italic><sup><italic>k</italic></sup> can be computed using its feature contrast to all other clusters:
<disp-formula id="pone.0182227.e010"><alternatives><graphic xlink:href="pone.0182227.e010.jpg" id="pone.0182227.e010g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M10"><mml:mrow><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:msub><mml:mrow><mml:mrow><mml:mo>&#x02016;</mml:mo><mml:mrow><mml:msup><mml:mi>&#x003bc;</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mi>&#x003bc;</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow><mml:mo>&#x02016;</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo>,</mml:mo></mml:math></alternatives><label>(4)</label></disp-formula>
where <italic>n</italic><sup><italic>i</italic></sup> and <italic>N</italic> represent the pixel number of cluster <italic>C</italic><sup><italic>i</italic></sup> and the whole image, respectively. <italic>u</italic><sup><italic>i</italic></sup> denotes the cluster center associated with the cluster <italic>C</italic><sup><italic>i</italic></sup>. It is obvious that the larger clusters play more important roles. This approach can strongly enhance the contrast of most dominant or large regions. Unlike [<xref rid="pone.0182227.ref048" ref-type="bibr">48</xref>], we do not compute spatial cues in that texts in images do not strictly satisfy &#x02018;central bias rule&#x02019; (i.e., the regions near the image center draw more attention than the other regions). We call the original MSER extraction on cluster-based saliency map as C-MSER for simplification.</p><p>We further apply color histogram-based contrast method inspired by [<xref rid="pone.0182227.ref049" ref-type="bibr">49</xref>] to enhance contrast of the small-size regions. Due to the fact that human vision cannot distinguish subtle difference between two similar colors, we reduce color numbers by color quantization which also greatly reduces the computational complexity of color differences computation. Cheng et al. [<xref rid="pone.0182227.ref049" ref-type="bibr">49</xref>] applies uniform quantization which uniformly quantizes each channel of RGB model to 12 different values. However, we employ minimum variance quantization proposed by Heckbert [<xref rid="pone.0182227.ref050" ref-type="bibr">50</xref>] because of the fact that uniform quantization does not take the non-uniform color distribution of a natural image into considerations. Minimum variance quantization constructs a new color map which allocates more entries to colors that appear frequently, and fewer ones to that appear infrequently [<xref rid="pone.0182227.ref051" ref-type="bibr">51</xref>]. Thus, small-size regions assigned with fewer entries in the output color map retain the differentiation and rarity. In this work, we quantize the 24-bit RGB input to 8-bit output with minimum variance quantization which reduces the number of colors to 256.</p><p>After quantization, we compute its color histogram by counting the numbers of each color in the RGB color space. Considering that colors in a natural image typically cover only a small portion of the full color space, we further abandon 5 percent of the image pixels whose colors occur less frequently. These pixels are replaced by the closest color in the histogram. While the quantization is performed in the RGB color space, color difference is computed in the <italic>L</italic>*<italic>a</italic>*<italic>b</italic>* color space. The saliency value of color <italic>c</italic><sub><italic>i</italic></sub> is defined as [<xref rid="pone.0182227.ref049" ref-type="bibr">49</xref>]:
<disp-formula id="pone.0182227.e011"><alternatives><graphic xlink:href="pone.0182227.e011.jpg" id="pone.0182227.e011g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M11"><mml:mrow><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></alternatives><label>(5)</label></disp-formula>
where <italic>D</italic>(<italic>c</italic><sub><italic>i</italic></sub>,<italic>c</italic><sub><italic>j</italic></sub>) is the color distance metric between color <italic>c</italic><sub><italic>i</italic></sub> and <italic>c</italic><sub><italic>j</italic></sub> in the <italic>L</italic>*<italic>a</italic>*<italic>b</italic>* space, <italic>n</italic> is the number of colors and <italic>f</italic><sub><italic>j</italic></sub> is the probability that color <italic>c</italic><sub><italic>j</italic></sub> occurs. In order to reduce noisy saliency results caused by color quantization, we smooth the saliency value of each color by replacing the weighted average of the saliency values of similar colors. The saliency value of color <italic>c</italic> can be defined [<xref rid="pone.0182227.ref049" ref-type="bibr">49</xref>]:
<disp-formula id="pone.0182227.e012"><alternatives><graphic xlink:href="pone.0182227.e012.jpg" id="pone.0182227.e012g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M12"><mml:mrow><mml:mi>S</mml:mi><mml:mo>'</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></alternatives><label>(6)</label></disp-formula>
where <italic>m</italic> is the number of the nearest colors and here we choose <italic>m</italic> = <italic>n</italic>/4. <inline-formula id="pone.0182227.e013"><alternatives><graphic xlink:href="pone.0182227.e013.jpg" id="pone.0182227.e013g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M13"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives></inline-formula> represents the sum of color difference between color <italic>c</italic> and its nearest colors <italic>c</italic><sub><italic>i</italic></sub>. Through smoothing, similar colors are more likely to be assigned similar saliency values, thus reducing quantization artifacts.</p><p>The original MSER algorithm is performed on both saliency maps. Finally the results on all channels compose the final character candidates for subsequent processing. We call this saliency- enhanced MSER as SE-MSER for simplification. Results on both saliency maps are illustrated in <xref ref-type="fig" rid="pone.0182227.g003">Fig 3</xref>.</p><fig id="pone.0182227.g003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0182227.g003</object-id><label>Fig 3</label><caption><title>Saliency maps.</title><p>(A) Original image [<xref rid="pone.0182227.ref053" ref-type="bibr">53</xref>]. (<bold>B</bold>) Cluster-based saliency map. (<bold>C</bold>) Histogram-based saliency map. Note the red rectangle in (<bold>C</bold>) illustrates the missing small characters in (<bold>B</bold>).</p></caption><graphic xlink:href="pone.0182227.g003"/></fig></sec></sec><sec id="sec008"><title>Character candidates filtering</title><sec id="sec009"><title>Deep text convolutional neural network</title><p>To reduce the number of false components detected by MSER, we seek a strong classifier to perform text/non-text classification. Convolutional neural network has been applied to a number of computer vision tasks with remarkable performance achieved in the last few years. Previous works [<xref rid="pone.0182227.ref005" ref-type="bibr">5</xref>, <xref rid="pone.0182227.ref018" ref-type="bibr">18</xref>, <xref rid="pone.0182227.ref021" ref-type="bibr">21</xref>] indicate that CNN is capable of learning meaningful high-level feature representations of text components. These approaches either train a character level CNN for scanning an image densely with sliding windows or generate a corresponding heat-map that indicates the probabilities of texts. Due to the fact that humans rely on character information to distinguish text and non-text, we apply a jointly trained deep model presented in [<xref rid="pone.0182227.ref021" ref-type="bibr">21</xref>] which incorporates pixel-level region information and character-level label information.</p><p>The structure of our convolutional text network is presented in <xref ref-type="fig" rid="pone.0182227.g004">Fig 4</xref>. An input image is first resized to 32&#x000d7;32 and then fed into the network, which is composed of three convolutional layers (with kernel size of 9&#x000d7;9, 7&#x000d7;7, 5&#x000d7;5, respectively) followed by two fully connected layers of size 1024. Each layer is followed by a Rectified Linear Unit (ReLU) as activation function. The second convolutional layer is followed by an additional max pooling layer with kernel 3&#x000d7;3. The last fully connected layer is followed by two softmax layers which perform text/non-text classification and 62-way character classification respectively. Another network branched from the second convolutional layer and composed of two deconvolution layers is the regression model.</p><fig id="pone.0182227.g004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0182227.g004</object-id><label>Fig 4</label><caption><title>Architecture of text CNN [<xref rid="pone.0182227.ref021" ref-type="bibr">21</xref>].</title></caption><graphic xlink:href="pone.0182227.g004"/></fig><p>The problem is formulated as a multi-task learning (MTL) problem with one main task (i.e., text/non-text classification) and two auxiliary tasks. Given an input image <italic>x</italic><sub><italic>i</italic></sub>, the goal of the MTL problem is to minimize
<disp-formula id="pone.0182227.e014"><alternatives><graphic xlink:href="pone.0182227.e014.jpg" id="pone.0182227.e014g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M14"><mml:mrow><mml:munder><mml:mrow><mml:mi mathvariant="normal">arg</mml:mi><mml:mspace width="0.20em"/><mml:mi mathvariant="normal">min</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mi>b</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi>r</mml:mi></mml:msup></mml:mrow></mml:munder><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mi>B</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mi>b</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi>b</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mi>L</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mi>R</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="normal">y</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi>r</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></alternatives><label>(7)</label></disp-formula>
where <italic>f</italic>(&#x02219;) is a function of <italic>x</italic><sub><italic>i</italic></sub> and parameterized by the weight vector <italic>w</italic>*. The loss function is denoted by <inline-formula id="pone.0182227.e015"><alternatives><graphic xlink:href="pone.0182227.e015.jpg" id="pone.0182227.e015g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M15"><mml:mrow><mml:mi>&#x02112;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x022c5;</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. <italic>&#x003bb;</italic><sub>*</sub> denotes the importance coefficient and the regularization terms are omitted for simplification. <inline-formula id="pone.0182227.e016"><alternatives><graphic xlink:href="pone.0182227.e016.jpg" id="pone.0182227.e016g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M16"><mml:mrow><mml:msup><mml:mi>&#x02112;</mml:mi><mml:mi>B</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0182227.e017"><alternatives><graphic xlink:href="pone.0182227.e017.jpg" id="pone.0182227.e017g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M17"><mml:mrow><mml:msup><mml:mi>&#x02112;</mml:mi><mml:mi>L</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0182227.e018"><alternatives><graphic xlink:href="pone.0182227.e018.jpg" id="pone.0182227.e018g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M18"><mml:mrow><mml:msup><mml:mi>&#x02112;</mml:mi><mml:mi>R</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> indicate text/non-text classification, character label classification and text region regression, respectively. <inline-formula id="pone.0182227.e019"><alternatives><graphic xlink:href="pone.0182227.e019.jpg" id="pone.0182227.e019g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M19"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mi>b</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> (i.e., text/non-text) is the label of the main task, <inline-formula id="pone.0182227.e020"><alternatives><graphic xlink:href="pone.0182227.e020.jpg" id="pone.0182227.e020g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M20"><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>&#x02026;</mml:mo><mml:mn>9</mml:mn><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>&#x02026;</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>&#x02026;</mml:mo><mml:mi>Z</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the label of the character classification task, and <inline-formula id="pone.0182227.e021"><alternatives><graphic xlink:href="pone.0182227.e021.jpg" id="pone.0182227.e021g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M21"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">y</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is 32&#x000d7;32 binary mask of the pixel-level text region. It is reasonable to employ the cross-entropy and least square as loss functions for classification tasks and regression task, respectively.</p><p>The training process is identical to [<xref rid="pone.0182227.ref021" ref-type="bibr">21</xref>]. After jointly training the two auxiliary tasks (i.e., text region regression and character recognition), we adopt &#x0201c;task-wise early stopping&#x0201d; method [<xref rid="pone.0182227.ref052" ref-type="bibr">52</xref>] to early stop the region task before the main task starts. The intuition is that low-level task will harm the main task after it reaches its peak performance as training proceeds. The character recognition task continues with training of the main task until the model is finally optimized.</p></sec><sec id="sec010"><title>Double threshold classification</title><p>The text CNN is adopted to filter non-text candidates among all the components detected by MSER. Inspired by previous work [<xref rid="pone.0182227.ref033" ref-type="bibr">33</xref>], the surviving character candidates are classified into three classes: strong text, weak text and non-text. In [<xref rid="pone.0182227.ref033" ref-type="bibr">33</xref>], Cho et al. applies a structure of two blocks of cascaded Adaboost classifiers, which is replaced by the more powerful text CNN, to filter the MSER candidates.</p><p>The cropped images of all candidates go through the CNN and the CNN produces a confident score for each of them. It is essential to filter out regions with a low confidence score and preserve those with a high score. This is accomplished by selecting high and low threshold values as follows:
<disp-formula id="pone.0182227.e022"><alternatives><graphic xlink:href="pone.0182227.e022.jpg" id="pone.0182227.e022g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M22"><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mi>&#x0211b;</mml:mi><mml:mi mathvariant="script">S</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&#x0003e;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>&#x0211b;</mml:mi><mml:mi mathvariant="script">W</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mi>&#x0211b;</mml:mi><mml:mi mathvariant="script">N</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:math></alternatives><label>(8)</label></disp-formula>
where <inline-formula id="pone.0182227.e023"><alternatives><graphic xlink:href="pone.0182227.e023.jpg" id="pone.0182227.e023g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M23"><mml:mrow><mml:msup><mml:mi>&#x0211b;</mml:mi><mml:mi mathvariant="script">S</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0182227.e024"><alternatives><graphic xlink:href="pone.0182227.e024.jpg" id="pone.0182227.e024g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M24"><mml:mrow><mml:msup><mml:mi>&#x0211b;</mml:mi><mml:mi mathvariant="script">W</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0182227.e025"><alternatives><graphic xlink:href="pone.0182227.e025.jpg" id="pone.0182227.e025g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M25"><mml:mrow><mml:msup><mml:mi>&#x0211b;</mml:mi><mml:mi mathvariant="script">N</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> indicate strong text, weak text and non-text, respectively. <italic>s</italic><sub><italic>c</italic></sub> is the confidence score, and <italic>T</italic><sub><italic>h</italic></sub>, <italic>T</italic><sub><italic>l</italic></sub> represent the high and low thresholds. Through this, the remaining regions can be separated as strong texts and weak texts, whereas the non-texts are removed from the candidates (see <xref ref-type="fig" rid="pone.0182227.g005">Fig 5</xref>). The double thresholds are determined by validation on the training set which satisfy precision of 99% and 90%, and here we set them to 0.995 and 0.978 respectively. <xref ref-type="fig" rid="pone.0182227.g006">Fig 6</xref> shows the classification results with the double threshold.</p><fig id="pone.0182227.g005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0182227.g005</object-id><label>Fig 5</label><caption><title>Examples of text candidate classification.</title><p>(A) Detected strong texts in green boxes. (B) Detected weak texts in blue boxes. (C) Strong text examples with confidence scores. (D) Weak text examples with confidence scores. (E) Non-text examples with confidence scores. The original image is from ICDAR 2011 dataset.</p></caption><graphic xlink:href="pone.0182227.g005"/></fig><fig id="pone.0182227.g006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0182227.g006</object-id><label>Fig 6</label><caption><title>Classification results by double threshold filtering.</title><p>(<bold>A</bold>) Original images [<xref rid="pone.0182227.ref053" ref-type="bibr">53</xref>]. (<bold>B</bold>) Strong texts. (<bold>C</bold>) Weak texts.</p></caption><graphic xlink:href="pone.0182227.g006"/></fig></sec></sec><sec id="sec011"><title>Text grouping and refinement</title><sec id="sec012"><title>Recursive neighborhood search</title><p>After candidates labeling, most of the non-text areas are removed while the text components are well preserved. However, some of the weak texts cannot be correctly classified due to low resolution or severe distortion. As nearby text areas share similar properties in the same word or text line, we propose a Recursive Neighborhood Searching (RNS) strategy to investigate positive components from the weak set. Details of the procedure are outlined in Algorithm 1.</p><boxed-text id="pone.0182227.box001" position="float" orientation="portrait"><p specific-use="line"><bold>Algorithm 1</bold> Recursive Neighborhood Search</p><p specific-use="line"><bold>Input:</bold> Strong text set <inline-formula id="pone.0182227.e026"><alternatives><graphic xlink:href="pone.0182227.e026.jpg" id="pone.0182227.e026g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M26"><mml:mrow><mml:msup><mml:mi>&#x0211b;</mml:mi><mml:mi mathvariant="script">S</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, weak text set <inline-formula id="pone.0182227.e027"><alternatives><graphic xlink:href="pone.0182227.e027.jpg" id="pone.0182227.e027g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M27"><mml:mrow><mml:msup><mml:mi>&#x0211b;</mml:mi><mml:mi mathvariant="script">W</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.</p><p specific-use="line"><bold>Output:</bold> Positive text set <inline-formula id="pone.0182227.e028"><alternatives><graphic xlink:href="pone.0182227.e028.jpg" id="pone.0182227.e028g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M28"><mml:mrow><mml:msup><mml:mi>&#x0211b;</mml:mi><mml:mi mathvariant="script">T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.</p><p specific-use="line">1: Initial <inline-formula id="pone.0182227.e029"><alternatives><graphic xlink:href="pone.0182227.e029.jpg" id="pone.0182227.e029g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M29"><mml:mrow><mml:msup><mml:mi>&#x0211b;</mml:mi><mml:mi mathvariant="script">S</mml:mi></mml:msup><mml:mo>&#x02190;</mml:mo><mml:msup><mml:mi>&#x0211b;</mml:mi><mml:mi mathvariant="script">S</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>;</p><p specific-use="line">2: <bold>for each</bold>
<inline-formula id="pone.0182227.e030"><alternatives><graphic xlink:href="pone.0182227.e030.jpg" id="pone.0182227.e030g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M30"><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211b;</mml:mi><mml:mi mathvariant="script">S</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> from left to right <bold>do</bold></p><p specific-use="line">3:&#x000a0;&#x000a0;<bold>for each</bold>
<inline-formula id="pone.0182227.e031"><alternatives><graphic xlink:href="pone.0182227.e031.jpg" id="pone.0182227.e031g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M31"><mml:mrow><mml:mi>l</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mi>&#x0211b;</mml:mi><mml:mi mathvariant="script">W</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
<bold>do</bold></p><p specific-use="line">4:&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>if</bold>
<italic>Dist</italic><sub><italic>cen</italic></sub>(<italic>c</italic>,<italic>l</italic>) &#x02264; <italic>T</italic><sub>1</sub> &#x000d7; max(<italic>w</italic><sub><italic>c</italic></sub>,<italic>w</italic><sub><italic>l</italic></sub>)</p><p specific-use="line">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x00026;<italic>abs</italic>(<italic>w</italic><sub><italic>c</italic></sub>,<italic>w</italic><sub><italic>l</italic></sub>) &#x0003c; min(<italic>w</italic><sub><italic>c</italic></sub>,<italic>w</italic><sub><italic>l</italic></sub>)</p><p specific-use="line">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x00026;<italic>abs</italic>(<italic>h</italic><sub><italic>c</italic></sub>,<italic>h</italic><sub><italic>l</italic></sub>) &#x0003c; min(<italic>h</italic><sub><italic>c</italic></sub>,<italic>h</italic><sub><italic>l</italic></sub>)</p><p specific-use="line">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x00026;<italic>Dist</italic><sub><italic>color</italic></sub>(<italic>c</italic>,<italic>l</italic>) &#x02264; <italic>T</italic><sub>2</sub></p><p specific-use="line">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x00026;1/<italic>T</italic><sub>3</sub> &#x0003c; <italic>sw</italic><sub><italic>c</italic></sub>/<italic>sw</italic><sub><italic>l</italic></sub> &#x0003c; <italic>T</italic><sub>3</sub></p><p specific-use="line">5:&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<inline-formula id="pone.0182227.e032"><alternatives><graphic xlink:href="pone.0182227.e032.jpg" id="pone.0182227.e032g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M32"><mml:mrow><mml:msup><mml:mi>&#x0211b;</mml:mi><mml:mi mathvariant="script">T</mml:mi></mml:msup><mml:mo>&#x02190;</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>;</p><p specific-use="line">6:&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;remove <italic>l</italic> from <inline-formula id="pone.0182227.e033"><alternatives><graphic xlink:href="pone.0182227.e033.jpg" id="pone.0182227.e033g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M33"><mml:mrow><mml:msup><mml:mi>&#x0211b;</mml:mi><mml:mi mathvariant="script">W</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>;</p><p specific-use="line">7:&#x000a0;&#x000a0;&#x000a0;<bold>endif</bold></p><p specific-use="line">8:&#x000a0;&#x000a0;<bold>endfor</bold></p><p specific-use="line">9:&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>if</bold>
<inline-formula id="pone.0182227.e034"><alternatives><graphic xlink:href="pone.0182227.e034.jpg" id="pone.0182227.e034g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M34"><mml:mrow><mml:msup><mml:mi>&#x0211b;</mml:mi><mml:mi mathvariant="script">W</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mo>&#x02205;</mml:mo></mml:mrow></mml:math></alternatives></inline-formula></p><p specific-use="line">10:&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>break</bold>;</p><p specific-use="line">11:&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>endif</bold></p><p specific-use="line">12: <bold>endfor</bold></p><p specific-use="line">Tracking texts by similar heuristic features.</p></boxed-text><p>Here <italic>Dist</italic><sub><italic>cen</italic></sub>(<italic>c</italic>,<italic>l</italic>) denotes normalized <italic>L</italic>2 distance between <italic>c</italic> and <italic>l</italic>, and <italic>Dist</italic><sub><italic>color</italic></sub>(<italic>c</italic>,<italic>l</italic>) stands for color difference. <italic>w</italic>, <italic>h</italic> and <italic>sw</italic> refer to width, height and mean stroke width respectively. The parameter <italic>T</italic><sub>1</sub> is set to 2, the threshold <italic>T</italic><sub>2</sub> for color difference is set to 25, and <italic>T</italic><sub>3</sub> is set to 1.5. <xref ref-type="fig" rid="pone.0182227.g001">Fig 1H</xref> shows the tracked texts by RNS.</p><p>In our experiments, there exist some inner boxes or outer boxes corresponding to the same text (e.g., partial detected and out of range), which may be a threat to the following text grouping procedure. To address the problem, we apply a robust method to solve this problem. In the first stage, we perform non-maximum suppression based on confident scores among overlapped regions and select the region with the highest score. In the second stage, we merge the region with the remaining regions if their intersection-over-union measure is greater than 0.5. The merging step is processed in descending order of confident scores.</p></sec><sec id="sec013"><title>Text Line Construction</title><p>Given the results of Recursive Neighborhood Searching, the text line construction is straightforward. Similar to previous work [<xref rid="pone.0182227.ref009" ref-type="bibr">9</xref>, <xref rid="pone.0182227.ref019" ref-type="bibr">19</xref>, <xref rid="pone.0182227.ref033" ref-type="bibr">33</xref>], we only use some commonly used heuristic rules to group the text candidates into lines. As our searching algorithm has extracted credible characters, the grouping procedure can be performed efficiently based on the same similarity measures (i.e., spatial location, size, color and aspect ratio). Candidates which satisfy the property similarities are grouped sequentially into the same word to construct the final text lines. Minimum bounding boxes that enclose grouped texts are our final results, as shown in <xref ref-type="fig" rid="pone.0182227.g001">Fig 1I</xref>.</p></sec></sec></sec><sec id="sec014"><title>Experiments and results</title><sec id="sec015"><title>Evaluation method and datasets</title><p>We evaluated the proposed method on two widely cited datasets for benchmarking scene text detection: ICDAR 2011 RRC dataset [<xref rid="pone.0182227.ref053" ref-type="bibr">53</xref>] (the dataset is downloaded from <ext-link ext-link-type="uri" xlink:href="http://www.cvc.uab.es/icdar2011competition">http://www.cvc.uab.es/icdar2011competition</ext-link>), and ICDAR 2013 RRC dataset [<xref rid="pone.0182227.ref017" ref-type="bibr">17</xref>] (the dataset is downloaded from <ext-link ext-link-type="uri" xlink:href="http://rrc.cvc.uab.es/">http://rrc.cvc.uab.es</ext-link>). The images of &#x0201c;Challenge 2: Reading Text in Scene Images&#x0201d; are used. The real world images consist of a range of scenes with texts and word-level ground truth is provided. We do not evaluate on more recent ICDAR 2015 dataset [<xref rid="pone.0182227.ref016" ref-type="bibr">16</xref>] as it is almost identical to the 2013 dataset.</p><p>We evaluate our algorithm from two aspects: character-level and word-level. For character-level recall evaluation, PASCAL VOC style protocol (i.e., a correct detection is determined if the intersection-over-union between a detected region and ground truth region is over 0.5) is used. For word-level evaluation, we adopt the protocol proposed by Wolf et al. [<xref rid="pone.0182227.ref054" ref-type="bibr">54</xref>]. In this criterion, the matching cases are classified as one-to-one, one-to-many and many-to-many.</p></sec><sec id="sec016"><title>Evaluation of the Saliency-enhanced MSER</title><p>We perform a quantitative analysis of character-level recall on the ICDAR 2011 dataset. The total number of images and characters in the test set are 255 and 6309, respectively. The character-level ground truth data is obtained from Cho H [<xref rid="pone.0182227.ref033" ref-type="bibr">33</xref>]. The character extraction is performed by original MSER algorithm on each channel and combined channels, and the number of candidates is counted additionally (see <xref ref-type="fig" rid="pone.0182227.g007">Fig 7A</xref>). The MSER threshold here is set to 1 to ensure a better recall. The result shows that the best recall rate is achieved on the combination of intensity (I), hue (H) and saturation (S) channels, and obviously the intensity channel plays the leading role.</p><fig id="pone.0182227.g007" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0182227.g007</object-id><label>Fig 7</label><caption><title>Evaluation of character-level recall on the ICDAR 2011 test set.</title><p>(<bold>A</bold>) Original MSER on multiple channels. (<bold>B</bold>) Multiple MSERs comparison.</p></caption><graphic xlink:href="pone.0182227.g007"/></fig><p>To evaluate the effectiveness of our saliency-enhanced MSER, we compare the recall using original MSER (MSER), cluster-based enhanced MSER (C-MSER) and saliency-enhance MSER (SE-MSER), respectively (see <xref ref-type="fig" rid="pone.0182227.g007">Fig 7B</xref>). The orange column indicates that 96.6% characters can be detected if all ERs are extracted, the total number of which is 6051331. Results without intensity channel are not shown for simplification. It is worth mentioning that our saliency-enhanced MSER achieves a comparable recall rate to all ERs with a much smaller number of candidates.</p><p><xref ref-type="table" rid="pone.0182227.t001">Table 1</xref> shows the comparison results with other character candidate extraction methods. Note that the evaluation method in the first four rows (i.e., a character is detected if the bounding box matches over 90% of the ground truth), which is slightly loose compared with the VOC protocol, is not the same as the rest. The evaluation method mentioned in [<xref rid="pone.0182227.ref007" ref-type="bibr">7</xref>, <xref rid="pone.0182227.ref029" ref-type="bibr">29</xref>] requires that over 90% of the detected area is matched by a ground truth box for the detection to be considered correct. Therefore, a small detected region contained in a large ground truth box will be considered positive. This problem can be solved by the VOC evaluation method which is based on intersection-over-union. It can be seen that our method using three channels achieves the highest recall rate. Note that we do not discuss the numbers here in that our aim is to extract as many true characters as we can for further processing.</p><table-wrap id="pone.0182227.t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0182227.t001</object-id><label>Table 1</label><caption><title>Character-level recall rate on the ICDAR 2011 test set.</title></caption><alternatives><graphic id="pone.0182227.t001g" xlink:href="pone.0182227.t001"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Algorithm</th><th align="center" rowspan="1" colspan="1">Recall (%)</th></tr></thead><tbody><tr><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">Neumann and Matas [<xref rid="pone.0182227.ref007" ref-type="bibr">7</xref>] (grayscale)</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">85.6</td></tr><tr><td align="center" rowspan="1" colspan="1">Neumann and Matas [<xref rid="pone.0182227.ref007" ref-type="bibr">7</xref>] (four channels)</td><td align="center" rowspan="1" colspan="1">93.7</td></tr><tr><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">Yin et al. [<xref rid="pone.0182227.ref029" ref-type="bibr">29</xref>] (grayscale)</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">90.2</td></tr><tr><td align="center" rowspan="1" colspan="1">Yin et al. [<xref rid="pone.0182227.ref029" ref-type="bibr">29</xref>](three channels)</td><td align="center" rowspan="1" colspan="1">95.2</td></tr><tr><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">Sung et al. [<xref rid="pone.0182227.ref034" ref-type="bibr">34</xref>] (three channels)</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">87.7</td></tr><tr><td align="center" rowspan="1" colspan="1">Sung et al. [<xref rid="pone.0182227.ref034" ref-type="bibr">34</xref>] (initial ERs)</td><td align="center" rowspan="1" colspan="1">89.6</td></tr><tr><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">Cho H et al. [<xref rid="pone.0182227.ref033" ref-type="bibr">33</xref>]</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">95.1</td></tr><tr><td align="center" rowspan="1" colspan="1">Our proposed method</td><td align="center" rowspan="1" colspan="1"><bold>95.53</bold></td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec017"><title>Experiments on ICDAR2011 and ICDAR 2013</title><p>We evaluate our method on the ICDAR 2011 dataset for word-level experiments. <xref ref-type="table" rid="pone.0182227.t002">Table 2</xref> illustrates the comparison results with some recently published methods. The proposed method achieves 0.836 in F-measure, outperforming other methods. Compared to the closest competitors [<xref rid="pone.0182227.ref019" ref-type="bibr">19</xref>, <xref rid="pone.0182227.ref033" ref-type="bibr">33</xref>], the precision of our algorithm (0.875) is much higher than both of theirs (0.76 and 0.71), which owes to the high accuracy of detected characters by the high threshold filtering.</p><table-wrap id="pone.0182227.t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0182227.t002</object-id><label>Table 2</label><caption><title>Experimental results on the ICDAR 2011 dataset.</title></caption><alternatives><graphic id="pone.0182227.t002g" xlink:href="pone.0182227.t002"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Method</th><th align="center" rowspan="1" colspan="1">Recall</th><th align="center" rowspan="1" colspan="1">Precision</th><th align="center" rowspan="1" colspan="1">F-score</th></tr></thead><tbody><tr><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">Yi et al. [<xref rid="pone.0182227.ref011" ref-type="bibr">11</xref>]</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.581</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.672</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.623</td></tr><tr><td align="center" rowspan="1" colspan="1">Epshtein et al. [<xref rid="pone.0182227.ref006" ref-type="bibr">6</xref>]</td><td align="center" rowspan="1" colspan="1">0.60</td><td align="center" rowspan="1" colspan="1">0.73</td><td align="center" rowspan="1" colspan="1">0.66</td></tr><tr><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">Kim et al. [<xref rid="pone.0182227.ref047" ref-type="bibr">47</xref>]</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.625</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.830</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.713</td></tr><tr><td align="center" rowspan="1" colspan="1">Shi et al. [<xref rid="pone.0182227.ref009" ref-type="bibr">9</xref>]</td><td align="center" rowspan="1" colspan="1">0.631</td><td align="center" rowspan="1" colspan="1">0.833</td><td align="center" rowspan="1" colspan="1">0.718</td></tr><tr><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">Yao et al. [<xref rid="pone.0182227.ref056" ref-type="bibr">56</xref>]</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.827</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.652</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.730</td></tr><tr><td align="center" rowspan="1" colspan="1">Neumann and Matas [<xref rid="pone.0182227.ref008" ref-type="bibr">8</xref>]</td><td align="center" rowspan="1" colspan="1">0.675</td><td align="center" rowspan="1" colspan="1">0.854</td><td align="center" rowspan="1" colspan="1">0.754</td></tr><tr><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">Yin et al. [<xref rid="pone.0182227.ref029" ref-type="bibr">29</xref>]</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.683</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.863</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.762</td></tr><tr><td align="center" rowspan="1" colspan="1">Huang et al. [<xref rid="pone.0182227.ref019" ref-type="bibr">19</xref>]</td><td align="center" rowspan="1" colspan="1"><bold>0.88</bold></td><td align="center" rowspan="1" colspan="1">0.71</td><td align="center" rowspan="1" colspan="1">0.78</td></tr><tr><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">Zhang et al. [<xref rid="pone.0182227.ref025" ref-type="bibr">25</xref>]</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.84</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.76</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.80</td></tr><tr><td align="center" rowspan="1" colspan="1">Proposed</td><td align="center" rowspan="1" colspan="1">0.801</td><td align="center" rowspan="1" colspan="1"><bold>0.875</bold></td><td align="center" rowspan="1" colspan="1"><bold>0.836</bold></td></tr></tbody></table></alternatives></table-wrap><p>The performances of the proposed algorithm as well as other methods on the ICDAR 2013 are depicted in <xref ref-type="table" rid="pone.0182227.t003">Table 3</xref>. The proposed method obtains 0.792, 0.884 and 0.835 in recall, precision and F-measure, respectively. The recall and F-measure still outperforms other methods. This confirms the effectiveness of our algorithm, especially its robustness in handling various scenarios.</p><table-wrap id="pone.0182227.t003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0182227.t003</object-id><label>Table 3</label><caption><title>Experimental results on the ICDAR 2013 dataset.</title></caption><alternatives><graphic id="pone.0182227.t003g" xlink:href="pone.0182227.t003"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="center" rowspan="1" colspan="1">Method</th><th align="center" rowspan="1" colspan="1">Recall</th><th align="center" rowspan="1" colspan="1">Precision</th><th align="center" rowspan="1" colspan="1">F-score</th></tr></thead><tbody><tr><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">Neumann and Matas [<xref rid="pone.0182227.ref008" ref-type="bibr">8</xref>]</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.648</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.875</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.745</td></tr><tr><td align="center" rowspan="1" colspan="1">USTB TexStar [<xref rid="pone.0182227.ref017" ref-type="bibr">17</xref>]</td><td align="center" rowspan="1" colspan="1">0.664</td><td align="center" rowspan="1" colspan="1">0.885</td><td align="center" rowspan="1" colspan="1">0.759</td></tr><tr><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">Neumann and Matas [<xref rid="pone.0182227.ref031" ref-type="bibr">31</xref>]</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.724</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.818</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.771</td></tr><tr><td align="center" rowspan="1" colspan="1">Zhang et al. [<xref rid="pone.0182227.ref025" ref-type="bibr">25</xref>]</td><td align="center" rowspan="1" colspan="1">0.74</td><td align="center" rowspan="1" colspan="1">0.88</td><td align="center" rowspan="1" colspan="1">0.80</td></tr><tr><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">Sung et al. [<xref rid="pone.0182227.ref034" ref-type="bibr">34</xref>]</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.742</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.887</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.808</td></tr><tr><td align="center" rowspan="1" colspan="1">He et al. [<xref rid="pone.0182227.ref021" ref-type="bibr">21</xref>]</td><td align="center" rowspan="1" colspan="1">0.723</td><td align="center" rowspan="1" colspan="1"><bold>0.923</bold></td><td align="center" rowspan="1" colspan="1">0.817</td></tr><tr><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">Cho et al. [<xref rid="pone.0182227.ref033" ref-type="bibr">33</xref>]</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.785</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.863</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.822</td></tr><tr><td align="center" rowspan="1" colspan="1">Zhang et al. [<xref rid="pone.0182227.ref057" ref-type="bibr">57</xref>]</td><td align="center" rowspan="1" colspan="1">0.78</td><td align="center" rowspan="1" colspan="1">0.88</td><td align="center" rowspan="1" colspan="1">0.83</td></tr><tr><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">Proposed</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1"><bold>0.792</bold></td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1">0.884</td><td align="center" style="background-color:#D9D9D9" rowspan="1" colspan="1"><bold>0.835</bold></td></tr></tbody></table></alternatives></table-wrap><p>Besides the quantitative experimental results, several typical detection examples of the proposed method are shown in <xref ref-type="fig" rid="pone.0182227.g008">Fig 8</xref> and <xref ref-type="fig" rid="pone.0182227.g009">Fig 9</xref>. As can been seen, our proposed algorithm works fairly well under various challenging cases (e.g., dot matrix fonts, low resolution and non-uniform illumination), which may not been handled perfectly using conventional methods.</p><fig id="pone.0182227.g008" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0182227.g008</object-id><label>Fig 8</label><caption><title>Detection examples of the proposed method on the ICDAR 2011 dataset [<xref rid="pone.0182227.ref053" ref-type="bibr">53</xref>].</title></caption><graphic xlink:href="pone.0182227.g008"/></fig><fig id="pone.0182227.g009" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0182227.g009</object-id><label>Fig 9</label><caption><title>Detection examples of the proposed method on the ICDAR 2013 dataset [<xref rid="pone.0182227.ref017" ref-type="bibr">17</xref>].</title></caption><graphic xlink:href="pone.0182227.g009"/></fig></sec><sec id="sec018"><title>Limitation of the proposed method</title><p>The proposed method is capable of dealing with several challenging cases and achieves excellent performance on standard benchmarks. However, there is still a great gap between our result and the perfect performance. Several failed examples are illustrated in <xref ref-type="fig" rid="pone.0182227.g010">Fig 10</xref>. As can be seen, false positive and missing characters may appear in certain situations, such as extremely low contrast, overexposure, special alignment, and blurring or tremendous gap between characters. Some cases have extremely ambiguous text information and are even hard for human to distinguish.</p><fig id="pone.0182227.g010" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0182227.g010</object-id><label>Fig 10</label><caption><title>Unsuccessful samples on the ICDAR dataset [<xref rid="pone.0182227.ref017" ref-type="bibr">17</xref>, <xref rid="pone.0182227.ref053" ref-type="bibr">53</xref>].</title></caption><graphic xlink:href="pone.0182227.g010"/></fig><p>Another limitation is the speed of the proposed algorithm. We implemented our algorithm in MATLAB and the CNN was trained in Caffe [<xref rid="pone.0182227.ref055" ref-type="bibr">55</xref>] framework. Our system was benchmarked on a 3.3GHz, 8 cores PC with a Quadro K4000 GPU, running Linux 64bit version. The average running time was evaluated on the ICDAR 2011. The original MSER implemented in MATLAB takes about 980ms and our SE-MSER takes about 3.7s per image. This can be accelerated significantly using C++ with more engineering optimization. The Text-CNN takes about 1.5s per image and the convolutional forward passes can be very fast running powerful GPU devices. The average time of processing one image is about 5.8s, which is slightly faster than the result (7.3s) reported in [<xref rid="pone.0182227.ref020" ref-type="bibr">20</xref>]. However, this speed is still far from the requirement of real-time applications. More efforts need to be made on speeding up the text proposal method.</p></sec></sec><sec sec-type="conclusions" id="sec019"><title>Conclusions</title><p>In this paper, we have introduced a novel algorithm for text detection in natural images. On the basis of previous successful methods, we extend the advantages of MSER and deep convolutional neural network. Moreover, a double threshold filtering strategy is adopted instead of conventional one-step classification to better capture true texts and remove non-text components. The experiments on the latest ICDAR datasets demonstrate that the proposed algorithm outperforms other competing methods in the literature. In the future work, we will investigate better strategies to reduce the number of proposals efficiently, thus satisfying the requirement to real-time applications.</p></sec></body><back><ack><p>We are grateful to Fenglei Xu, Bingwen Hu for helpful discussions. Our thanks also go to Ling Gu for her kind proofreading of this manuscript.</p></ack><ref-list><title>References</title><ref id="pone.0182227.ref001"><label>1</label><mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>X</given-names></name>, <name><surname>Yuille</surname><given-names>AL</given-names></name>. <article-title>Detecting and reading text in natural scenes</article-title>. <source>IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2004</year>: <fpage>366</fpage>&#x02013;<lpage>73</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref002"><label>2</label><mixed-citation publication-type="journal"><name><surname>Kim</surname><given-names>KI</given-names></name>, <name><surname>Jung</surname><given-names>K</given-names></name>, <name><surname>Kim</surname><given-names>JH</given-names></name>. <article-title>Texture-based approach for text detection in images using support vector machines and continuously adaptive mean shift algorithm</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2003</year>; <volume>25</volume>(<issue>12</issue>): <fpage>1631</fpage>&#x02013;<lpage>9</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref003"><label>3</label><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>K</given-names></name>, <name><surname>Babenko</surname><given-names>B</given-names></name>, <name><surname>Belongie</surname><given-names>S</given-names></name>. <article-title>End-to-end scene text recognition</article-title>. <source>IEEE International Conference on Computer Vision</source>. <year>2011</year>: <fpage>1457</fpage>&#x02013;<lpage>64</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref004"><label>4</label><mixed-citation publication-type="journal"><name><surname>Neumann</surname><given-names>L</given-names></name>, <name><surname>Matas</surname><given-names>J</given-names></name>. <article-title>Scene text localization and recognition with oriented stroke detection</article-title>. <source>IEEE International Conference on Computer Vision</source>. <year>2013</year>: <fpage>97</fpage>&#x02013;<lpage>104</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/iccv.2013.19">10.1109/iccv.2013.19</ext-link></comment></mixed-citation></ref><ref id="pone.0182227.ref005"><label>5</label><mixed-citation publication-type="journal"><name><surname>Jaderberg</surname><given-names>M</given-names></name>, <name><surname>Vedaldi</surname><given-names>A</given-names></name>, <name><surname>Zisserman</surname><given-names>A</given-names></name>. <article-title>Deep features for text spotting</article-title>. <source>European Conference on Computer Vision</source>. <year>2014</year>; <volume>8692</volume>: <fpage>512</fpage>&#x02013;<lpage>28</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref006"><label>6</label><mixed-citation publication-type="journal"><name><surname>Epshtein</surname><given-names>B</given-names></name>, <name><surname>Ofek</surname><given-names>E</given-names></name>, <name><surname>Wexler</surname><given-names>Y</given-names></name>. <article-title>Detecting text in natural scenes with stroke width transform</article-title>. <source>IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2010</year>: <fpage>2963</fpage>&#x02013;<lpage>70</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/cvpr.2010.5540041">10.1109/cvpr.2010.5540041</ext-link></comment></mixed-citation></ref><ref id="pone.0182227.ref007"><label>7</label><mixed-citation publication-type="journal"><name><surname>Neumann</surname><given-names>L</given-names></name>, <name><surname>Matas</surname><given-names>J</given-names></name>. <article-title>Real-time scene text localization and recognition</article-title>. <source>IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2012</year>: <fpage>3538</fpage>&#x02013;<lpage>45</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref008"><label>8</label><mixed-citation publication-type="journal"><name><surname>Neumann</surname><given-names>L</given-names></name>, <name><surname>Matas</surname><given-names>J</given-names></name>. <article-title>On combining multiple segmentations in scene text recognition</article-title>. <source>International Conference on Document Analysis and Recognition</source>. <year>2013</year>: <fpage>523</fpage>&#x02013;<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/icdar.2013.110">10.1109/icdar.2013.110</ext-link></comment></mixed-citation></ref><ref id="pone.0182227.ref009"><label>9</label><mixed-citation publication-type="journal"><name><surname>Shi</surname><given-names>CZ</given-names></name>, <name><surname>Wang</surname><given-names>CH</given-names></name>, <name><surname>Xiao</surname><given-names>BH</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Gao</surname><given-names>S</given-names></name>. <article-title>Scene text detection using graph model built upon maximally stable extremal regions</article-title>. <source>Pattern Recognition Letters</source>. <year>2013</year>; <volume>34</volume>(<issue>2</issue>): <fpage>107</fpage>&#x02013;<lpage>16</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.patrec.2012.09.019">10.1016/j.patrec.2012.09.019</ext-link></comment></mixed-citation></ref><ref id="pone.0182227.ref010"><label>10</label><mixed-citation publication-type="journal"><name><surname>Yao</surname><given-names>C</given-names></name>, <name><surname>Bai</surname><given-names>X</given-names></name>, <name><surname>Liu</surname><given-names>WY</given-names></name>, <name><surname>Ma</surname><given-names>Y</given-names></name>, <name><surname>Tu</surname><given-names>ZW</given-names></name>. <article-title>Detecting texts of arbitrary orientations in natural images</article-title>. <source>IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2012</year>: <fpage>1083</fpage>&#x02013;<lpage>90</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref011"><label>11</label><mixed-citation publication-type="journal"><name><surname>Yi</surname><given-names>CC</given-names></name>, <name><surname>Tian</surname><given-names>YL</given-names></name>. <article-title>Text string detection from natural scenes by structure-based partition and grouping</article-title>. <source>IEEE Transactions on Image Processing</source>. <year>2011</year>; <volume>20</volume>(<issue>9</issue>): <fpage>2594</fpage>&#x02013;<lpage>605</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TIP.2011.2126586">10.1109/TIP.2011.2126586</ext-link></comment>
<pub-id pub-id-type="pmid">21411405</pub-id></mixed-citation></ref><ref id="pone.0182227.ref012"><label>12</label><mixed-citation publication-type="journal"><name><surname>Yi</surname><given-names>CC</given-names></name>, <name><surname>Tian</surname><given-names>YL</given-names></name>. <article-title>Text extraction from scene images by character appearance and structure modeling</article-title>. <source>Computer Vision and Image Understanding</source>. <year>2013</year>; <volume>117</volume>(<issue>2</issue>): <fpage>182</fpage>&#x02013;<lpage>94</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cviu.2012.11.002">10.1016/j.cviu.2012.11.002</ext-link></comment>
<pub-id pub-id-type="pmid">23316111</pub-id></mixed-citation></ref><ref id="pone.0182227.ref013"><label>13</label><mixed-citation publication-type="journal"><name><surname>Huang</surname><given-names>WL</given-names></name>, <name><surname>Lin</surname><given-names>Z</given-names></name>, <name><surname>Yang</surname><given-names>JC</given-names></name>, <name><surname>Wang</surname><given-names>J</given-names></name>. <article-title>Text localization in natural images using stroke feature transform and text covariance descriptors</article-title>. <source>IEEE International Conference on Computer Vision</source>. <year>2013</year>: <fpage>1241</fpage>&#x02013;<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/iccv.2013.157">10.1109/iccv.2013.157</ext-link></comment></mixed-citation></ref><ref id="pone.0182227.ref014"><label>14</label><mixed-citation publication-type="journal"><name><surname>Neumann</surname><given-names>L</given-names></name>, <name><surname>Matas</surname><given-names>J</given-names></name>. <article-title>Text localization in real-world images using efficiently pruned exhaustive search</article-title>. <source>International Conference on Document Analysis and Recognition</source>. <year>2011</year>: <fpage>687</fpage>&#x02013;<lpage>91</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/icdar.2011.144">10.1109/icdar.2011.144</ext-link></comment></mixed-citation></ref><ref id="pone.0182227.ref015"><label>15</label><mixed-citation publication-type="journal"><name><surname>Neumann</surname><given-names>L</given-names></name>, <name><surname>Matas</surname><given-names>J</given-names></name>. <article-title>A method for text localization and recognition in real-world images</article-title>. <source>Asian Conference on Computer Vision</source>. <year>2011</year>; <volume>6494</volume>: <fpage>770</fpage>&#x02013;<lpage>83</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref016"><label>16</label><mixed-citation publication-type="journal"><name><surname>Karatzas</surname><given-names>D</given-names></name>, <name><surname>Lu</surname><given-names>S</given-names></name>, <name><surname>Shafait</surname><given-names>F</given-names></name>, <name><surname>Uchida</surname><given-names>S</given-names></name>, <name><surname>Valveny</surname><given-names>E</given-names></name>, <name><surname>Gomezbigorda</surname><given-names>L</given-names></name>, <etal>et al</etal>
<article-title>ICDAR 2015 competition on robust reading</article-title>. <source>International Conference on Document Analysis and Recognition</source>. <year>2015</year>: <fpage>1150</fpage>&#x02013;<lpage>60</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref017"><label>17</label><mixed-citation publication-type="journal"><name><surname>Karatzas</surname><given-names>D</given-names></name>, <name><surname>Shafait</surname><given-names>F</given-names></name>, <name><surname>Uchida</surname><given-names>S</given-names></name>, <name><surname>Iwamura</surname><given-names>M</given-names></name>. <article-title>ICDAR 2013 robust reading competition</article-title>. <source>International Conference on Document Analysis and Recognition</source>. <year>2013</year>: <fpage>1484</fpage>&#x02013;<lpage>93</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref018"><label>18</label><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>T</given-names></name>, <name><surname>Wu</surname><given-names>DJ</given-names></name>, <name><surname>Coates</surname><given-names>A</given-names></name>, <name><surname>Ng</surname><given-names>AY</given-names></name>. <article-title>End-to-end text recognition with convolutional neural networks</article-title>. <source>International Conference on Pattern Recognition</source>. <year>2012</year>: <fpage>3304</fpage>&#x02013;<lpage>8</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref019"><label>19</label><mixed-citation publication-type="journal"><name><surname>Huang</surname><given-names>W</given-names></name>, <name><surname>Qiao</surname><given-names>Y</given-names></name>, <name><surname>Tang</surname><given-names>X</given-names></name>. <article-title>Robust scene text detection with convolution neural network induced MSER trees</article-title>. <source>European Conference on Computer Vision</source>. <year>2014</year>; <volume>8692</volume>: <fpage>497</fpage>&#x02013;<lpage>511</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref020"><label>20</label><mixed-citation publication-type="journal"><name><surname>Jaderberg</surname><given-names>M</given-names></name>, <name><surname>Simonyan</surname><given-names>K</given-names></name>, <name><surname>Vedaldi</surname><given-names>A</given-names></name>, <name><surname>Zisserman</surname><given-names>A</given-names></name>. <article-title>Reading text in the wild with convolutional neural networks</article-title>. <source>International Journal of Computer Vision</source>. <year>2016</year>; <volume>116</volume>(<issue>1</issue>): <fpage>1</fpage>&#x02013;<lpage>20</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s11263-015-0823-z">10.1007/s11263-015-0823-z</ext-link></comment></mixed-citation></ref><ref id="pone.0182227.ref021"><label>21</label><mixed-citation publication-type="journal"><name><surname>He</surname><given-names>T</given-names></name>, <name><surname>Huang</surname><given-names>W</given-names></name>, <name><surname>Qiao</surname><given-names>Y</given-names></name>, <name><surname>Yao</surname><given-names>J</given-names></name>. <article-title>Text-attentional convolutional neural network for scene text detection</article-title>. <source>IEEE Transactions on Image Processing</source>. <year>2016</year>; <volume>25</volume>(<issue>6</issue>): <fpage>2529</fpage>&#x02013;<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TIP.2016.2547588">10.1109/TIP.2016.2547588</ext-link></comment>
<pub-id pub-id-type="pmid">27093723</pub-id></mixed-citation></ref><ref id="pone.0182227.ref022"><label>22</label><mixed-citation publication-type="journal"><name><surname>Tian</surname><given-names>Z</given-names></name>, <name><surname>Huang</surname><given-names>W</given-names></name>, <name><surname>He</surname><given-names>T</given-names></name>, <name><surname>He</surname><given-names>P</given-names></name>, <name><surname>Qiao</surname><given-names>Y</given-names></name>. <article-title>Detecting text in natural image with connectionist text proposal network</article-title>. <source>European Conference on Computer Vision</source>. <year>2016</year>: <fpage>56</fpage>&#x02013;<lpage>72</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/978-3-319-46484-8_4">10.1007/978-3-319-46484-8_4</ext-link></comment></mixed-citation></ref><ref id="pone.0182227.ref023"><label>23</label><mixed-citation publication-type="journal"><name><surname>Gupta</surname><given-names>A</given-names></name>, <name><surname>Vedaldi</surname><given-names>A</given-names></name>, <name><surname>Zisserman</surname><given-names>A</given-names></name>. <article-title>Synthetic data for text localisation in natural images</article-title>. <source>IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2016</year>: <fpage>2315</fpage>&#x02013;<lpage>24</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref024"><label>24</label><mixed-citation publication-type="other">Jaderberg M, Simonyan K, Vedaldi A, Zisserman A. Synthetic data and artificial neural networks for natural scene text recognition; 2014. Preprint. Available from: arXiv:1406.2227.</mixed-citation></ref><ref id="pone.0182227.ref025"><label>25</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Z</given-names></name>, <name><surname>Wei</surname><given-names>S</given-names></name>, <name><surname>Yao</surname><given-names>C</given-names></name>, <name><surname>Bai</surname><given-names>X</given-names></name>. <article-title>Symmetry-based text line detection in natural scenes</article-title>. <source>IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2015</year>: <fpage>2558</fpage>&#x02013;<lpage>67</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/CVPR.2015.7298871">10.1109/CVPR.2015.7298871</ext-link></comment></mixed-citation></ref><ref id="pone.0182227.ref026"><label>26</label><mixed-citation publication-type="journal"><name><surname>Sun</surname><given-names>L</given-names></name>, <name><surname>Huo</surname><given-names>Q</given-names></name>, <name><surname>Jia</surname><given-names>W</given-names></name>, <name><surname>Chen</surname><given-names>K</given-names></name>. <article-title>Robust text detection in natural scene images by generalized color-enhanced contrasting extremal region and neural networks</article-title>. <source>International Conference on Pattern Recognition</source>. <year>2014</year>: <fpage>2715</fpage>&#x02013;<lpage>20</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/icpr.2014.469">10.1109/icpr.2014.469</ext-link></comment></mixed-citation></ref><ref id="pone.0182227.ref027"><label>27</label><mixed-citation publication-type="journal"><name><surname>Sun</surname><given-names>L</given-names></name>, <name><surname>Huo</surname><given-names>Q</given-names></name>, <name><surname>Jia</surname><given-names>W</given-names></name>, <name><surname>Chen</surname><given-names>K</given-names></name>. <article-title>A robust approach for text detection from natural scene images</article-title>. <source>Pattern Recognition</source>. <year>2015</year>; <volume>48</volume>(<issue>9</issue>): <fpage>2906</fpage>&#x02013;<lpage>20</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.patcog.2015.04.002">10.1016/j.patcog.2015.04.002</ext-link></comment></mixed-citation></ref><ref id="pone.0182227.ref028"><label>28</label><mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>G</given-names></name>, <name><surname>Liu</surname><given-names>J</given-names></name>, <name><surname>Zhang</surname><given-names>S</given-names></name>, <name><surname>Zheng</surname><given-names>Y</given-names></name>. <article-title>Scene text detection with extremal region based cascaded filtering</article-title>. <source>IEEE International Conference on Image Processing</source>. <year>2016</year>: <fpage>2896</fpage>&#x02013;<lpage>900</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/ICIP.2016.7532889">10.1109/ICIP.2016.7532889</ext-link></comment></mixed-citation></ref><ref id="pone.0182227.ref029"><label>29</label><mixed-citation publication-type="journal"><name><surname>Yin</surname><given-names>XC</given-names></name>, <name><surname>Yin</surname><given-names>XW</given-names></name>, <name><surname>Huang</surname><given-names>KZ</given-names></name>, <name><surname>Hao</surname><given-names>HW</given-names></name>. <article-title>Robust text detection in natural scene images</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2014</year>; <volume>36</volume>(<issue>5</issue>): <fpage>970</fpage>&#x02013;<lpage>83</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TPAMI.2013.182">10.1109/TPAMI.2013.182</ext-link></comment>
<pub-id pub-id-type="pmid">26353230</pub-id></mixed-citation></ref><ref id="pone.0182227.ref030"><label>30</label><mixed-citation publication-type="journal"><name><surname>Busta</surname><given-names>M</given-names></name>, <name><surname>Neumann</surname><given-names>L</given-names></name>, <name><surname>Matas</surname><given-names>J</given-names></name>. <article-title>FASText: efficient unconstrained scene text detector</article-title>. <source>IEEE International Conference on Computer Vision</source>. <year>2015</year>: <fpage>1206</fpage>&#x02013;<lpage>14</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/iccv.2015.143">10.1109/iccv.2015.143</ext-link></comment></mixed-citation></ref><ref id="pone.0182227.ref031"><label>31</label><mixed-citation publication-type="journal"><name><surname>Neumann</surname><given-names>L</given-names></name>, <name><surname>Matas</surname><given-names>J</given-names></name>. <article-title>Efficient scene text localization and recognition with local character refinement</article-title>. <source>International Conference on Document Analysis and Recognition</source>. <year>2015</year>: <fpage>746</fpage>&#x02013;<lpage>50</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/ICDAR.2015.7333861">10.1109/ICDAR.2015.7333861</ext-link></comment></mixed-citation></ref><ref id="pone.0182227.ref032"><label>32</label><mixed-citation publication-type="journal"><name><surname>Matas</surname><given-names>J</given-names></name>, <name><surname>Chum</surname><given-names>O</given-names></name>, <name><surname>Urban</surname><given-names>M</given-names></name>, <name><surname>Pajdla</surname><given-names>T</given-names></name>. <article-title>Robust wide-baseline stereo from maximally stable extremal regions</article-title>. <source>Image and Vision Computing</source>. <year>2004</year>; <volume>22</volume>(<issue>10</issue>): <fpage>761</fpage>&#x02013;<lpage>7</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref033"><label>33</label><mixed-citation publication-type="journal"><name><surname>Cho</surname><given-names>H</given-names></name>, <name><surname>Sung</surname><given-names>M</given-names></name>, <name><surname>Jun</surname><given-names>B</given-names></name>. <article-title>Canny text detector: fast and robust scene text localization algorithm</article-title>. <source>IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2016</year>: <fpage>3566</fpage>&#x02013;<lpage>73</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref034"><label>34</label><mixed-citation publication-type="journal"><name><surname>Sung</surname><given-names>MC</given-names></name>, <name><surname>Jun</surname><given-names>B</given-names></name>, <name><surname>Cho</surname><given-names>H</given-names></name>, <name><surname>Kim</surname><given-names>D</given-names></name>. <article-title>Scene text detection with robust character candidate extraction method</article-title>. <source>International Conference on Document Analysis and Recognition</source>. <year>2015</year>: <fpage>426</fpage>&#x02013;<lpage>30</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/ICDAR.2015.7333797">10.1109/ICDAR.2015.7333797</ext-link></comment></mixed-citation></ref><ref id="pone.0182227.ref035"><label>35</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>B</given-names></name>, <name><surname>Perina</surname><given-names>A</given-names></name>, <name><surname>Li</surname><given-names>Z</given-names></name>, <name><surname>Murino</surname><given-names>V</given-names></name>, <name><surname>Liu</surname><given-names>J</given-names></name>, <name><surname>Ji</surname><given-names>R</given-names></name>. <article-title>Bounding multiple gaussians uncertainty with application to object tracking</article-title>. <source>International Journal of Computer Vision</source>. <year>2016</year>; <volume>118</volume>(<issue>3</issue>): <fpage>364</fpage>&#x02013;<lpage>79</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref036"><label>36</label><mixed-citation publication-type="journal"><name><surname>L</surname><given-names>G</given-names></name>, <name><surname>D</surname><given-names>K</given-names></name>. <article-title>Multi-script text extraction from natural scenes</article-title>. <source>International Conference on Document Analysis and Recognition</source>. <year>2013</year>: <fpage>467</fpage>&#x02013;<lpage>71</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/ICDAR.2013.100">10.1109/ICDAR.2013.100</ext-link></comment></mixed-citation></ref><ref id="pone.0182227.ref037"><label>37</label><mixed-citation publication-type="journal"><name><surname>Jing</surname><given-names>XY</given-names></name>, <name><surname>Zhang</surname><given-names>D</given-names></name>. <article-title>A face and palmprint recognition approach based on discriminant DCT feature extraction</article-title>. <source>IEEE Transactions on Systems Man &#x00026; Cybernetics Part B Cybernetics A Publication of the IEEE Systems Man &#x00026; Cybernetics Society</source>. <year>2004</year>; <volume>34</volume>(<issue>6</issue>): <fpage>2405</fpage>&#x02013;<lpage>15</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref038"><label>38</label><mixed-citation publication-type="journal"><name><surname>Jing</surname><given-names>XY</given-names></name>, <name><surname>Yao</surname><given-names>YF</given-names></name>, <name><surname>Zhang</surname><given-names>D</given-names></name>, <name><surname>Yang</surname><given-names>JY</given-names></name>, <name><surname>Li</surname><given-names>M</given-names></name>. <article-title>Face and palmprint pixel level fusion and kernel DCV-RBF classifier for small sample biometric recognition</article-title>. <source>Pattern Recognition</source>. <year>2007</year>; <volume>40</volume>(<issue>11</issue>): <fpage>3209</fpage>&#x02013;<lpage>24</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref039"><label>39</label><mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>JH</given-names></name>, <name><surname>Su</surname><given-names>H</given-names></name>, <name><surname>Yi</surname><given-names>YH</given-names></name>, <name><surname>Hu</surname><given-names>WB</given-names></name>. <article-title>Robust text detection via multi-degree of sharpening and blurring</article-title>. <source>Signal Processing</source>. <year>2016</year>; <volume>124</volume>: <fpage>259</fpage>&#x02013;<lpage>65</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.sigpro.2015.06.025">10.1016/j.sigpro.2015.06.025</ext-link></comment></mixed-citation></ref><ref id="pone.0182227.ref040"><label>40</label><mixed-citation publication-type="journal"><name><surname>Shi</surname><given-names>CZ</given-names></name>, <name><surname>Wang</surname><given-names>CH</given-names></name>, <name><surname>Xiao</surname><given-names>BH</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Gao</surname><given-names>S</given-names></name>, <name><surname>Zhang</surname><given-names>Z</given-names></name>. <article-title>Scene text recognition using part-based tree-structured character detection</article-title>. <source>IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2013</year>: <fpage>2961</fpage>&#x02013;<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/cvpr.2013.381">10.1109/cvpr.2013.381</ext-link></comment></mixed-citation></ref><ref id="pone.0182227.ref041"><label>41</label><mixed-citation publication-type="journal"><name><surname>Kang</surname><given-names>L</given-names></name>, <name><surname>Li</surname><given-names>Y</given-names></name>, <name><surname>Doermann</surname><given-names>D</given-names></name>. <article-title>Orientation robust text line detection in natural images</article-title>. <source>IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2014</year>: <fpage>4034</fpage>&#x02013;<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/cvpr.2014.514">10.1109/cvpr.2014.514</ext-link></comment></mixed-citation></ref><ref id="pone.0182227.ref042"><label>42</label><mixed-citation publication-type="journal"><name><surname>Jing</surname><given-names>XY</given-names></name>, <name><surname>Zhu</surname><given-names>X</given-names></name>, <name><surname>Wu</surname><given-names>F</given-names></name>, <name><surname>You</surname><given-names>X</given-names></name>, <name><surname>Liu</surname><given-names>Q</given-names></name>, <name><surname>Yue</surname><given-names>D</given-names></name>, <etal>et al</etal>
<article-title>Super-resolution person re-identification with semi-coupled low-rank discriminant dictionary learning</article-title>. <source>IEEE Transactions on Image Processing</source>. <year>2017</year>; <volume>26</volume>(<issue>3</issue>): <fpage>1363</fpage>&#x02013;<lpage>78</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TIP.2017.2651364">10.1109/TIP.2017.2651364</ext-link></comment>
<pub-id pub-id-type="pmid">28092535</pub-id></mixed-citation></ref><ref id="pone.0182227.ref043"><label>43</label><mixed-citation publication-type="journal"><name><surname>Zhao</surname><given-names>M</given-names></name>, <name><surname>Li</surname><given-names>S</given-names></name>, <name><surname>Kwok</surname><given-names>J</given-names></name>. <article-title>Text detection in images using sparse representation with discriminative dictionaries</article-title>. <source>Image &#x00026; Vision Computing</source>. <year>2010</year>; <volume>28</volume>(<issue>12</issue>): <fpage>1590</fpage>&#x02013;<lpage>9</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref044"><label>44</label><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>L</given-names></name>, <name><surname>Zhang</surname><given-names>B</given-names></name>, <name><surname>Han</surname><given-names>J</given-names></name>, <name><surname>Shen</surname><given-names>L</given-names></name>, <name><surname>Qian</surname><given-names>CS</given-names></name>. <article-title>Robust object representation by boosting-like deep learning architecture</article-title>. <source>Signal Processing Image Communication</source>. <year>2016</year>; <volume>47</volume>: <fpage>490</fpage>&#x02013;<lpage>9</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref045"><label>45</label><mixed-citation publication-type="journal"><name><surname>Chong</surname><given-names>HY</given-names></name>, <name><surname>Gortler</surname><given-names>SJ</given-names></name>, <name><surname>Zickler</surname><given-names>T</given-names></name>. <article-title>A perception-based color space for illumination-invariant image processing</article-title>. <source>ACM Transactions on Graphics</source>. <year>2008</year>; <volume>27</volume>(<issue>3</issue>): <fpage>15</fpage>&#x02013;<lpage>9</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref046"><label>46</label><mixed-citation publication-type="journal"><name><surname>Donoser</surname><given-names>M</given-names></name>, <name><surname>Bischof</surname><given-names>H</given-names></name>. <article-title>Efficient maximally stable extremal region (MSER) tracking</article-title>. <source>IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2006</year>: <fpage>553</fpage>&#x02013;<lpage>60</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref047"><label>47</label><mixed-citation publication-type="journal"><name><surname>Koo</surname><given-names>HI</given-names></name>, <name><surname>Kim</surname><given-names>DH</given-names></name>. <article-title>Scene text detection via connected component clustering and nontext filtering</article-title>. <source>IEEE Transactions on Image Processing</source>. <year>2013</year>; <volume>22</volume>(<issue>6</issue>): <fpage>2296</fpage>&#x02013;<lpage>305</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TIP.2013.2249082">10.1109/TIP.2013.2249082</ext-link></comment>
<pub-id pub-id-type="pmid">23475363</pub-id></mixed-citation></ref><ref id="pone.0182227.ref048"><label>48</label><mixed-citation publication-type="journal"><name><surname>Fu</surname><given-names>H</given-names></name>, <name><surname>Cao</surname><given-names>X</given-names></name>, <name><surname>Tu</surname><given-names>Z</given-names></name>. <article-title>Cluster-based co-saliency detection</article-title>. <source>IEEE Transactions on Image Processing</source>. <year>2013</year>; <volume>22</volume>(<issue>10</issue>): <fpage>3766</fpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TIP.2013.2260166">10.1109/TIP.2013.2260166</ext-link></comment>
<pub-id pub-id-type="pmid">23629857</pub-id></mixed-citation></ref><ref id="pone.0182227.ref049"><label>49</label><mixed-citation publication-type="journal"><name><surname>Cheng</surname><given-names>MM</given-names></name>, <name><surname>Zhang</surname><given-names>GX</given-names></name>, <name><surname>Mitra</surname><given-names>NJ</given-names></name>, <name><surname>Huang</surname><given-names>X</given-names></name>, <name><surname>Hu</surname><given-names>SM</given-names></name>. <article-title>Global contrast based salient region detection</article-title>. <source>IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2011</year>: <fpage>409</fpage>&#x02013;<lpage>16</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref050"><label>50</label><mixed-citation publication-type="journal"><name><surname>Heckbert</surname><given-names>P</given-names></name>. <article-title>Color image quantization for frame buffer display</article-title>. <source>ACM Siggraph Computer Graphics</source>. <year>1980</year>; <volume>16</volume>(<issue>3</issue>): <fpage>297</fpage>&#x02013;<lpage>307</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref051"><label>51</label><mixed-citation publication-type="journal"><name><surname>Lou</surname><given-names>J</given-names></name>, <name><surname>Ren</surname><given-names>M</given-names></name>, <name><surname>Wang</surname><given-names>H</given-names></name>. <article-title>Regional principal color based saliency detection</article-title>. <source>Plos One</source>. <year>2014</year>; <volume>9</volume>(<issue>11</issue>): <fpage>e112475</fpage>
<comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0112475">10.1371/journal.pone.0112475</ext-link></comment>
<pub-id pub-id-type="pmid">25379960</pub-id></mixed-citation></ref><ref id="pone.0182227.ref052"><label>52</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Z</given-names></name>, <name><surname>Luo</surname><given-names>P</given-names></name>, <name><surname>Chen</surname><given-names>CL</given-names></name>, <name><surname>Tang</surname><given-names>X</given-names></name>. <article-title>Facial landmark detection by deep multi-task learning</article-title>. <source>European Conference on Computer Vision</source>. <year>2014</year>: <fpage>94</fpage>&#x02013;<lpage>108</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref053"><label>53</label><mixed-citation publication-type="journal"><name><surname>Shahab</surname><given-names>A</given-names></name>, <name><surname>Shafait</surname><given-names>F</given-names></name>, <name><surname>Dengel</surname><given-names>A</given-names></name>. <article-title>ICDAR 2011 robust reading competition challenge 2: reading text in scene images</article-title>. <source>International Conference on Document Analysis and Recognition</source>. <year>2011</year>: <fpage>1491</fpage>&#x02013;<lpage>6</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref054"><label>54</label><mixed-citation publication-type="journal"><name><surname>Wolf</surname><given-names>C</given-names></name>, <name><surname>Jolion</surname><given-names>JM</given-names></name>. <article-title>Object count/area graphs for the evaluation of object detection and segmentation algorithms</article-title>. <source>International Journal on Document Analysis and Recognition</source>. <year>2006</year>; <volume>8</volume>(<issue>4</issue>): <fpage>280</fpage>&#x02013;<lpage>96</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref055"><label>55</label><mixed-citation publication-type="journal"><name><surname>Jia</surname><given-names>Y</given-names></name>, <name><surname>Shelhamer</surname><given-names>E</given-names></name>, <name><surname>Donahue</surname><given-names>J</given-names></name>, <name><surname>Karayev</surname><given-names>S</given-names></name>, <name><surname>Long</surname><given-names>J</given-names></name>, <name><surname>Girshick</surname><given-names>R</given-names></name>, <etal>et al</etal>
<article-title>Caffe: convolutional architecture for fast feature embedding</article-title>. <source>ACM International Conference on Multimedia</source>. <year>2014</year>: <fpage>675</fpage>&#x02013;<lpage>8</lpage>.</mixed-citation></ref><ref id="pone.0182227.ref056"><label>56</label><mixed-citation publication-type="journal"><name><surname>Yao</surname><given-names>C</given-names></name>, <name><surname>Bai</surname><given-names>X</given-names></name>, <name><surname>Liu</surname><given-names>WY</given-names></name>. <article-title>A unified framework for multioriented text detection and recognition</article-title>. <source>IEEE Transactions on Image Processing</source>. <year>2014</year>; <volume>23</volume>(<issue>11</issue>): <fpage>4737</fpage>&#x02013;<lpage>49</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TIP.2014.2353813">10.1109/TIP.2014.2353813</ext-link></comment>
<pub-id pub-id-type="pmid">25203989</pub-id></mixed-citation></ref><ref id="pone.0182227.ref057"><label>57</label><mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Z</given-names></name>, <name><surname>Zhang</surname><given-names>C</given-names></name>, <name><surname>Shen</surname><given-names>W</given-names></name>, <name><surname>Yao</surname><given-names>C</given-names></name>, <name><surname>Liu</surname><given-names>W</given-names></name>, <name><surname>Bai</surname><given-names>X</given-names></name>. <article-title>Multi-oriented text detection with fully convolutional networks</article-title>. <source>IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2016</year>: <fpage>4159</fpage>&#x02013;<lpage>67</lpage>.</mixed-citation></ref></ref-list></back></article>